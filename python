attention_timeseries_forecasting.py
# ============================================================
# Advanced Time Series Forecasting with Attention Mechanisms
# Transformer vs ARIMA Baseline
# ============================================================

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.arima.model import ARIMA

# ======================
# CONFIG
# ======================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)

SEQ_LEN = 48
PRED_LEN = 12
BATCH_SIZE = 64
EPOCHS = 40
LR = 1e-3
MODEL_DIM = 64
NUM_HEADS = 4
NUM_LAYERS = 2

# ======================
# SYNTHETIC DATA GENERATION
# ======================
def generate_time_series(n_steps=3000):
    t = np.arange(n_steps)

    trend = 0.005 * t + 0.3 * np.sin(0.005 * t**1.1)
    seasonal = 2 * np.sin(2 * np.pi * t / 50) + np.sin(2 * np.pi * t / 100)
    noise = np.random.normal(scale=0.3 + 0.002 * t, size=n_steps)

    series = trend + seasonal + noise

    # regime shift
    series[n_steps // 2:] += 5 * np.sin(0.02 * t[n_steps // 2:])
    return series

series = generate_time_series()

# ======================
# PREPROCESSING
# ======================
scaler = StandardScaler()
series_scaled = scaler.fit_transform(series.reshape(-1, 1)).flatten()

def create_windows(data, seq_len, pred_len):
    X, y = [], []
    for i in range(len(data) - seq_len - pred_len):
        X.append(data[i:i + seq_len])
        y.append(data[i + seq_len:i + seq_len + pred_len])
    return np.array(X), np.array(y)

X, y = create_windows(series_scaled, SEQ_LEN, PRED_LEN)

split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# ======================
# DATASET
# ======================
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_loader = DataLoader(
    TimeSeriesDataset(X_train, y_train),
    batch_size=BATCH_SIZE,
    shuffle=True
)

test_loader = DataLoader(
    TimeSeriesDataset(X_test, y_test),
    batch_size=BATCH_SIZE,
    shuffle=False
)

# ======================
# TRANSFORMER MODEL
# ======================
class TransformerForecast(nn.Module):
    def __init__(self):
        super().__init__()

        self.input_proj = nn.Linear(1, MODEL_DIM)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=MODEL_DIM,
            nhead=NUM_HEADS,
            dim_feedforward=128,
            dropout=0.1,
            batch_first=True
        )

        self.encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=NUM_LAYERS
        )

        self.output = nn.Linear(MODEL_DIM, PRED_LEN)

    def forward(self, x):
        x = x.unsqueeze(-1)
        x = self.input_proj(x)
        h = self.encoder(x)
        return self.output(h[:, -1])

model = TransformerForecast().to(DEVICE)
optimizer = optim.Adam(model.parameters(), lr=LR)
loss_fn = nn.MSELoss()

# ======================
# TRAINING
# ======================
for epoch in range(EPOCHS):
    model.train()
    total_loss = 0

    for Xb, yb in train_loader:
        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)

        preds = model(Xb)
        loss = loss_fn(preds, yb)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch [{epoch+1}/{EPOCHS}] MSE: {total_loss / len(train_loader):.4f}")

# ======================
# EVALUATION
# ======================
model.eval()
preds_dl = []

with torch.no_grad():
    for Xb, _ in test_loader:
        Xb = Xb.to(DEVICE)
        preds_dl.append(model(Xb).cpu().numpy())

preds_dl = np.concatenate(preds_dl).flatten()
y_true = y_test.flatten()

rmse_dl = np.sqrt(mean_squared_error(y_true, preds_dl))

# ======================
# MASE
# ======================
def mase(y_true, y_pred, insample):
    naive_forecast = np.mean(np.abs(np.diff(insample)))
    return np.mean(np.abs(y_true - y_pred)) / naive_forecast

mase_dl = mase(y_true, preds_dl, series_scaled[:split])

# ======================
# ARIMA BASELINE
# ======================
arima = ARIMA(series_scaled[:split], order=(5,1,0))
arima_fit = arima.fit()

arima_forecast = arima_fit.forecast(steps=len(y_true))
rmse_arima = np.sqrt(mean_squared_error(y_true, arima_forecast))
mase_arima = mase(y_true, arima_forecast, series_scaled[:split])

# ======================
# RESULTS
# ======================
print("\n===== PERFORMANCE COMPARISON =====")
print(f"Transformer RMSE: {rmse_dl:.4f}")
print(f"Transformer MASE: {mase_dl:.4f}")
print("----------------------------------")
print(f"ARIMA RMSE:       {rmse_arima:.4f}")
print(f"ARIMA MASE:       {mase_arima:.4f}")
